{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of the data from the [Di-Tech Challenge](http://research.xiaojukeji.com/competition), organized by Didi Chuxing, a ride-hailing company in China. The data is described [here](http://research.xiaojukeji.com/competition/detail.action?competitionId=DiTech2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Are you running for the final submission?\n",
    "final_submission = False\n",
    "# final_submission = True\n",
    "\n",
    "# Paths\n",
    "path_to_data = 'data/season_1/'\n",
    "# test_set_number = '1'\n",
    "test_set_number = '2'\n",
    "path_to_test = path_to_data + 'test_set_' + test_set_number + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import clock\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Warn about chained assignment in pandas?\n",
    "# pd.options.mode.chained_assignment = None\n",
    "\n",
    "if not final_submission:\n",
    "    \n",
    "    # import matplotlib\n",
    "    # matplotlib.use('Agg')\n",
    "\n",
    "    from ggplot import *\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Info Table\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>order_id</td>\n",
    "            <td>string</td>\n",
    "            <td>order ID</td>\n",
    "            <td>70fc7c2bd2caf386bb50f8fd5dfef0cf</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>driver_id</td>\n",
    "            <td>string</td>\n",
    "            <td>driver ID</td>\n",
    "            <td>56018323b921dd2c5444f98fb45509de</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>passenger_id</td>\n",
    "            <td>string</td>\n",
    "            <td>user ID</td>\n",
    "            <td>238de35f44bbe8a67bdea86a5b0f4719</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>start_district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>departure</td>\n",
    "            <td>d4ec2125aff74eded207d2d915ef682f</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>dest_district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>destination</td>\n",
    "            <td>929ec6c160e6f52c20a4217c7978f681</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Price</td>\n",
    "            <td>double</td>\n",
    "            <td>Price</td>\n",
    "            <td>37.5</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>string</td>\n",
    "            <td>Timestamp of the order</td>\n",
    "            <td>2016-01-15 00:35:11</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The Order Info Table shows the basic information of an order, including the passenger and the driver (if driver_id =NULL, it means the order was not answered by any driver), place of origin, destination, price and time. The fields order_id, driver_id, passenger_id, start_hash, and dest_hash are made not sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_orders(order_files):\n",
    "    \"\"\"\n",
    "    Create data frames for order data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Columns in order files\n",
    "    columns = ['order_id', 'driver_id', 'passenger_id', \n",
    "               'start_district_hash', 'dest_district_hash', 'price', 'time']\n",
    "\n",
    "    # Open all of them\n",
    "    order_dfs = []\n",
    "    for order_file in order_files:\n",
    "        order_dfs.append(pd.read_csv(order_file, sep = \"\\t\", names = columns))\n",
    "    df = pd.concat(order_dfs)\n",
    "\n",
    "    # Recognize time column as time\n",
    "    df['time'] = pd.to_datetime(df.time)\n",
    "    # Set the row labels to the time stamp\n",
    "    # df = df.set_index('time')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Files are organized by dates\n",
    "if final_submission:\n",
    "    order_files = glob.glob(path_to_data + 'training_data/order_data/*')\n",
    "else:\n",
    "    order_files = [path_to_data + \"training_data/order_data/order_data_2016-01-{:02d}\".format(i) \n",
    "                   for i in range(10, 16)]\n",
    "\n",
    "df = load_orders(order_files)\n",
    "\n",
    "# Keep first two weeks for training, next one week for validation.\n",
    "ind = df['time'] < pd.to_datetime('2016-01-15')\n",
    "df_train = df[ind]\n",
    "df_valid = df[~ind]\n",
    "\n",
    "# Keep a random number of the rows\n",
    "# df_train = df.sample(frac = 0.70, random_state = 111)\n",
    "# df_valid = df[~df.index.isin(df_train.index)]\n",
    "\n",
    "# Avoid looking at validation set during the exploration\n",
    "df = df_train\n",
    "\n",
    "# Quick look at the data frame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dates?\n",
    "\n",
    "print(\"Dates from {} to {}.\".format(df['time'].min(), df['time'].max()))\n",
    "\n",
    "# How many entries? unique orders/passengers/drivers?\n",
    "\n",
    "num_entries = df.shape[0]\n",
    "print(\"{} entries\".format(num_entries))\n",
    "\n",
    "num_orders = len(df.order_id.unique())\n",
    "print(\"{} unique orders ({:.1%})\".format(num_orders, num_orders/num_entries))\n",
    "\n",
    "num_pass = len(df.passenger_id.unique())\n",
    "print(\"{} unique passengers ({:.1%})\".format(num_pass, num_pass/num_entries))\n",
    "      \n",
    "num_drivers = len(df.driver_id.unique())\n",
    "print(\"{} unique drivers ({:.1%})\".format(num_drivers, num_drivers/num_entries))\n",
    "\n",
    "num_start_district = len(df.start_district_hash.unique())\n",
    "print(\"{} unique starting districts ({:.1%})\".format(num_start_district, num_start_district/num_entries))\n",
    "\n",
    "num_dest_district = len(df.dest_district_hash.unique())\n",
    "print(\"{} unique destination districts ({:.1%})\".format(num_dest_district, num_dest_district/num_entries))\n",
    "\n",
    "# Price distribution\n",
    "\n",
    "print(\"\\n\")\n",
    "print(df.price.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Attach time-related info \n",
    "\n",
    "def compute_timeinfo(df, dateonly = False):\n",
    "    # Input: data frame with time (or date) column.\n",
    "    \n",
    "    if dateonly:\n",
    "        # Determine day of week, weekend.\n",
    "        df['dow'] = pd.to_datetime(df.date).dt.dayofweek\n",
    "        df['weekend'] = df.dow >= 5\n",
    "    else:\n",
    "        # Extract the date, and implicitly make the time midnight.\n",
    "        df['date'] = df.time.dt.date\n",
    "        df['timeonly'] = df.time.dt.time\n",
    "        \n",
    "        # Compute 10-min timeslots -- absolute and per day.\n",
    "        df['timeslot_absolute'] = (df['time'] - pd.to_datetime('2016-01-01')).astype('timedelta64[m]')//10\n",
    "        df['timeslot_day'] = df['timeslot_absolute'] % (24 * 6)\n",
    "        df['timeslot'] = df['timeslot_day'] % (24*6) + 1\n",
    "    \n",
    "        # Determine day of week, weekend.\n",
    "        df['dow'] = df.time.dt.dayofweek\n",
    "        df['weekend'] = df.dow >= 5\n",
    "    \n",
    "        # Drop the time column?\n",
    "        # df = df.drop('time', axis = 1)\n",
    "    \n",
    "compute_timeinfo(df)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot of gap vs elapsed time in days group by ten minutes interval\n",
    "\n",
    "# Convert to days\n",
    "df['time_from_begin'] = df['timeslot_absolute']/6/24\n",
    "\n",
    "# Count how many rows per timeslot\n",
    "# count = df[['order_id', 'driver_id']].groupby('order_id').count()\n",
    "count = df.groupby('time_from_begin')['time_from_begin'].count()\n",
    "count = pd.DataFrame(count)\n",
    "count.columns = ['count']\n",
    "count.reset_index(level=0, inplace=True)\n",
    "\n",
    "if not final_submission:\n",
    "    ggplot(aes('time_from_begin', 'count'), data = count) + \\\n",
    "        geom_point(color = 'gray') + \\\n",
    "        geom_line() + \\\n",
    "        xlab('Elapsed time in days') + ylab('Order count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ranking districts\n",
    "districts_count = df['start_district_hash'].value_counts(sort = True)\n",
    "\n",
    "# Number of most popular districts to keep\n",
    "k = 10\n",
    "num_entries = df.shape[0]\n",
    "num_top = districts_count[:10].sum()\n",
    "\n",
    "# Look at k top districts\n",
    "print(\"The first {} most popular districts account for {} out of {} ({:.1%})\".format(\n",
    "        k, num_top, num_entries, num_top/num_entries))\n",
    "\n",
    "# Rank districts\n",
    "districts_rank = districts_count.rank(ascending = False, method = 'dense')\n",
    "districts_top = districts_rank[districts_rank <= k].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count how many rows per order_id and driver_id\n",
    "# count = df[['order_id', 'driver_id']].groupby('order_id').count()\n",
    "# count = count.reset_index()\n",
    "# count = count['driver_id']\n",
    "\n",
    "# Orders picked up by more than one driver?\n",
    "# print(\"Number of orders taken by more than one driver: {}\".format(sum(count > 1)))\n",
    "# Yes..? Surprising.\n",
    "\n",
    "# Turns out there are duplicate and almost-duplicate entries. \n",
    "# The FAQ recommends just leaving them in.\n",
    "\n",
    "# Remove the duplicates\n",
    "# dup = df.duplicated(['order_id', 'driver_id', 'passenger_id', 'time'], keep = 'last')\n",
    "# df = df[~dup]\n",
    "\n",
    "# Was order answered?\n",
    "df['is_gap'] = df['driver_id'].isnull()\n",
    "\n",
    "# Proportion of orders not picked up by a driver\n",
    "s = sum(df['is_gap'])\n",
    "l = len(df['is_gap'])\n",
    "print(\"There are {} orders-without-drivers out of {} orders: {:.1%}.\".format(s, l, s/l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot of gap vs elapsed time in days group by ten minutes interval\n",
    "\n",
    "cols = ['time_from_begin']\n",
    "df_select = df[cols + ['is_gap']]\n",
    "df_gap = df_select.groupby(cols).sum()\n",
    "df_gap = df_gap.reset_index()\n",
    "\n",
    "if not final_submission:\n",
    "    ggplot(aes('time_from_begin', 'is_gap'), data = df_gap) + \\\n",
    "        geom_point(color = 'gray') + \\\n",
    "        geom_line() + \\\n",
    "        xlab('Elapsed time in days') + ylab('Gap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_prior_gap(df):\n",
    "    \"\"\"\n",
    "    Find the gap for the three prior time slots. \n",
    "    Using shift is fast, but might not lead to the timeslots corresponding to the last 30 min.\n",
    "    \"\"\"\n",
    "    \n",
    "    # df.sort_values(by = 'timeslot_absolute', inplace = True)\n",
    "    df.sort_values(by = ['date', 'timeslot'], inplace = True)\n",
    "    df_grouped = df.groupby(['start_district_hash'])\n",
    "    \n",
    "    df['gap_before_1'] = df_grouped.shift(1).gap.fillna(method = 'bfill')\n",
    "    df['gap_before_2'] = df_grouped.shift(2).gap.fillna(method = 'bfill')\n",
    "    df['gap_before_3'] = df_grouped.shift(3).gap.fillna(method = 'bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make new data frame with gap per timeslot per district\n",
    "\n",
    "def compute_gap(df):\n",
    "    \n",
    "    # Compute gap column\n",
    "    # df['gap'] = df.groupby(cols).driver_id.isnull().transform('sum')\n",
    "    \n",
    "    # Gap is when no driver\n",
    "    df['is_gap'] = df['driver_id'].isnull()\n",
    "    \n",
    "    # Aggregate\n",
    "    cols = ['start_district_hash', 'date', 'timeslot']\n",
    "    df_select = df[cols + ['is_gap']]\n",
    "    df_gap = df_select.groupby(cols).sum()\n",
    "    \n",
    "    # Flatten data frame\n",
    "    df_gap = df_gap.reset_index()\n",
    "    df_gap = df_gap.rename(columns = {'is_gap': 'gap'})\n",
    "    \n",
    "    # Add prior gap columns\n",
    "    find_prior_gap(df_gap)\n",
    "    \n",
    "    return df_gap\n",
    "\n",
    "# Compute gap. Only care about new data frame now.\n",
    "df = compute_gap(df)\n",
    "\n",
    "# Sanity check: do the number add up?\n",
    "print(sum(df.gap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>string</td>\n",
    "            <td>Timestamp</td>\n",
    "            <td>2016-01-15 00:35:11</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Weather</td>\n",
    "            <td>int</td>\n",
    "            <td>Weather</td>\n",
    "            <td>7</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>temperature</td>\n",
    "            <td>double</td>\n",
    "            <td>Temperature</td>\n",
    "            <td>-9</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>PM2.5</td>\n",
    "            <td>double</td>\n",
    "            <td>pm25</td>\n",
    "            <td>66</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The Weather Info Table shows the weather info every 10 minutes each city. The weather field gives the weather conditions such as sunny, rainy, and snowy etc; all sensitive information has been removed. The unit of temperature is Celsius degree, and PM2.5 is the level of air pollutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_weather(weather_files):\n",
    "\n",
    "    # Open all of them\n",
    "    columns = ['time', 'weather', 'temperature', 'pm25']\n",
    "    weather_dfs = []\n",
    "    for f in weather_files:\n",
    "        weather_dfs.append(pd.read_csv(f, sep = \"\\t\", names = columns))\n",
    "    dfw = pd.concat(weather_dfs)\n",
    "\n",
    "    # Extract date and time slot\n",
    "    dfw['time'] = pd.to_datetime(dfw.time)\n",
    "    dfw['date'] = dfw.time.dt.date\n",
    "    dfw['timeslot'] = (dfw['time'] - pd.to_datetime(dfw['date'])).astype('timedelta64[m]')//10 + 1\n",
    "    dfw = dfw.drop('time', axis = 1)\n",
    "\n",
    "    return dfw\n",
    "\n",
    "# Files are organized by dates\n",
    "weather_files = glob.glob(path_to_data + 'training_data/weather_data/*')\n",
    "dfw = load_weather(weather_files)\n",
    "# dfw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_weather(df, dfw):\n",
    "    # Merge into main data frame, and fill missing values\n",
    "    # http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
    "    \n",
    "    df = df.merge(dfw, on = ['date', 'timeslot'], how = 'left')\n",
    "    \n",
    "    # Fill missing values with forward/backward fills\n",
    "    df.sort_values(by = ['date', 'timeslot'], inplace = True)\n",
    "    df.temperature = df.temperature.fillna(method = 'ffill')\n",
    "    df.temperature = df.temperature.fillna(method = 'bfill')\n",
    "    \n",
    "    # ... or interpolation\n",
    "    # df.temperature = df.temperature.interpolate(method = 'time')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = merge_weather(df, dfw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create conversion table\n",
    "districts_convert = pd.DataFrame(data = districts_rank)\n",
    "districts_convert.reset_index(level = 0, inplace = True)\n",
    "districts_convert.columns = ['start_district_hash', 'start_district_rank']\n",
    "# districts_convert.head(2)\n",
    "\n",
    "# Replace hash by rank\n",
    "df = df.merge(districts_convert, on = 'start_district_hash', how = 'left')\n",
    "# df = df.drop('start_district_hash', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical column for most-popular district hash.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Look only at most popular districts\n",
    "    df['district'] = np.nan\n",
    "    df.loc[df.start_district_hash.isin(districts_top), 'district'] = \\\n",
    "        df.loc[df.start_district_hash.isin(districts_top), 'start_district_hash']\n",
    "    \n",
    "    # One-hot encoding\n",
    "    dummies = pd.get_dummies(df['district'], dummy_na = False)\n",
    "    df = pd.concat((df.drop('district', axis = 1), dummies.astype(int)), axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# df = one_hot_encode(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# District Info Table\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>District hash</td>\n",
    "            <td>90c5a34f06ac86aee0fd70e2adce7d8a</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>district_id</td>\n",
    "            <td>string</td>\n",
    "            <td>District ID</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The District Info Table shows the information about the districts to be evaluated in the contest. You need to do the prediction given the districts from the District Definition Table. In the submission of the results, you need to map the district hash value to district mapped ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the starting district_hash as the associated disctrict\n",
    "col = 'start_district_hash'\n",
    "\n",
    "# Load district conversion table\n",
    "district_file = path_to_data + 'training_data/cluster_map/cluster_map'\n",
    "district = pd.read_csv(district_file, sep = '\\t', names = [col, 'district_id'])\n",
    "\n",
    "# How many districts?\n",
    "print(\"There are {} districts in the district file.\".format(district.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Columns in order files\n",
    "columns = ['datetimeslot']\n",
    "\n",
    "# Open list of slots\n",
    "order_file = path_to_test + 'read_me_' + test_set_number + '.txt'\n",
    "df_test_slots = pd.read_csv(order_file, sep = \"\\t\", names = columns, skiprows = 1)\n",
    "\n",
    "# Extract date and timeslot [0,143]\n",
    "df_test_slots['date'] = pd.to_datetime(df_test_slots.datetimeslot.str[:10])\n",
    "df_test_slots['timeslot'] = df_test_slots.datetimeslot.str[11:]\n",
    "df_test_slots['timeslot'] = df_test_slots.timeslot.astype(int)\n",
    "compute_timeinfo(df_test_slots, dateonly = True)\n",
    "\n",
    "# Create all combination of districts and time slots\n",
    "df_test_slots['to_predict'] = True\n",
    "district['to_predict'] = True\n",
    "df_test_slots = df_test_slots.merge(district, on = 'to_predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test order files\n",
    "order_files = glob.glob(path_to_test + 'order_data/*')\n",
    "df_test = load_orders(order_files)\n",
    "\n",
    "# Attach time info\n",
    "compute_timeinfo(df_test)\n",
    "\n",
    "# Compute gap per time slot per district\n",
    "df_test = compute_gap(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge with slots to predict in test set\n",
    "df_test['to_predict'] = False\n",
    "df_test_full = pd.concat([df_test, df_test_slots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "order_files = glob.glob(path_to_test + 'weather_data/*')\n",
    "dfw_test = load_weather(weather_files)\n",
    "\n",
    "# Get all the weather data\n",
    "dfw_all = pd.concat([dfw, dfw_test])\n",
    "\n",
    "# Merge weather data\n",
    "df_test_full = merge_weather(df_test_full, dfw_all)\n",
    "\n",
    "# Replace district by popularity\n",
    "df_test_full = df_test_full.merge(districts_convert, on = 'start_district_hash', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test_full.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute prior gap, and then keep only the data we need to predict\n",
    "find_prior_gap(df_test_full)\n",
    "\n",
    "# Re-divide the two data frames\n",
    "df_test = df_test[~df_test_full['to_predict']]\n",
    "df_test_slots = df_test_full[df_test_full['to_predict']]\n",
    "\n",
    "# Drop extra columns\n",
    "df_test.drop('to_predict', axis = 1, inplace = True)\n",
    "df_test_slots.drop('to_predict', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute timeslot\n",
    "compute_timeinfo(df_valid)\n",
    "\n",
    "# Compute gap per time slot per district\n",
    "df_valid = compute_gap(df_valid)\n",
    "\n",
    "# Merge temperature\n",
    "df_valid = merge_weather(df_valid, dfw)\n",
    "\n",
    "# One-hot encoding of districts\n",
    "# df_valid = one_hot_encode(df_valid)\n",
    "\n",
    "# Replace district by popularity\n",
    "df_valid = df_valid.merge(districts_convert, on = 'start_district_hash', how = 'left')\n",
    "\n",
    "# Extract timeslots for testing in validation\n",
    "timeslots = df_test_slots.timeslot.unique()\n",
    "df_valid = df_valid[df_valid['timeslot'].isin(timeslots)]\n",
    "\n",
    "# Check data frame out\n",
    "df_valid.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is it a weekend or weekday?\n",
    "compute_timeinfo(df, dateonly = True)\n",
    "compute_timeinfo(df_valid, dateonly = True)\n",
    "compute_timeinfo(df_test, dateonly = True)\n",
    "\n",
    "# Concatenate\n",
    "df_all = pd.concat([df, df_valid, df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions by Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class predict_by_cluster():\n",
    "    # Make predictions by simply taking the median per start_district_hash per timeslot per weekend\n",
    "    \n",
    "    # Select columns\n",
    "    cols = ['start_district_hash', 'weekend', 'timeslot']\n",
    "    # cols = ['start_district_hash', 'dow', 'timeslot']\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \n",
    "        # Make prediction data frame\n",
    "        df = df[self.cols + ['gap']]\n",
    "        df_grouped = df.groupby(self.cols)\n",
    "        gap_cluster = df_grouped.median() # median, mean, min, max\n",
    "        gap_cluster = gap_cluster.reset_index().rename(columns = {'gap': 'gap_cluster'})\n",
    "        \n",
    "        self.gap_cluster = gap_cluster\n",
    "\n",
    "    def merge(self, df):\n",
    "        \n",
    "        # Merge gap_cluster in data frame\n",
    "        df = df.merge(self.gap_cluster, on = self.cols, how = 'left')\n",
    "\n",
    "        # Fill missing predictions\n",
    "        df['gap_cluster'] = df['gap_cluster'].fillna(method = 'ffill')\n",
    "        df['gap_cluster'] = df['gap_cluster'].fillna(method = 'bfill')\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Make predictions\n",
    "gap_cluster = predict_by_cluster(df)\n",
    "\n",
    "# Make prediction table per group with ALL the data -- for final submission\n",
    "gap_cluster_all = predict_by_cluster(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge back into main data frame\n",
    "df = gap_cluster.merge(df)\n",
    "\n",
    "# Set given outcome and predictions\n",
    "train_outcome = df['gap']\n",
    "train_predict = df['gap_cluster']\n",
    "\n",
    "# Merge back into main data frame\n",
    "df_valid = gap_cluster.merge(df_valid)\n",
    "\n",
    "# Set given outcome and predictions\n",
    "valid_outcome = df['gap']\n",
    "valid_predict = df['gap_cluster']\n",
    "\n",
    "# Set predictions on test slots\n",
    "df_test_slots = gap_cluster_all.merge(df_test_slots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select features\n",
    "cols = ['start_district_rank', 'dow', 'timeslot', 'temperature', 'gap_before_1', 'gap_before_2', 'gap_before_3']    \n",
    "train = df_all[cols] if final_submission else df[cols]\n",
    "# train.head(5)\n",
    "\n",
    "# Fill missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer()\n",
    "train = imputer.fit_transform(train)\n",
    "\n",
    "# Reference outcome\n",
    "train_outcome = df_all['gap'] if final_submission else df['gap']\n",
    "# train_outcome.head(5)\n",
    "\n",
    "# print(pd.isnull(train).any(1).nonzero()[0])\n",
    "# print(train[pd.isnull(train).any(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor(n_estimators = 10)\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# reg = DecisionTreeRegressor(max_depth = 3)\n",
    "\n",
    "# Fit training data\n",
    "start = clock()\n",
    "reg.fit(train, train_outcome)\n",
    "print(\"Fit in {:.0f} seconds.\".format(clock() - start))\n",
    "\n",
    "# Variable importance\n",
    "# print(pd.DataFrame(data = [reg.feature_importances_], columns = train.columns, index = ['importance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test_slots.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extrapolate\n",
    "train_predict = reg.predict(train)\n",
    "\n",
    "# Validation\n",
    "valid_outcome = df_valid['gap']\n",
    "valid_predict = reg.predict(df_valid[cols])\n",
    "\n",
    "# Set predictions on test slots\n",
    "df_test_slots_imputed = imputer.transform(df_test_slots[cols])\n",
    "# df_test_slots_imputed = df_test_slots[cols]\n",
    "df_test_slots['gap_predict'] = reg.predict(df_test_slots_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Consider di districts and tj time slots, and the supply-demand gap gapij , and your prediction is sij, we use as the evaluation metrics: \n",
    "![MAPE](figures/mape.jpg)\n",
    "The lowest MAPE will be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(outcome, predict):\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Percentage Error (MAPE) score. Lower is better.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verify that there are no missing values\n",
    "    try:\n",
    "        assert not (outcome.isnull().values.any() or predict.isnull().values.any())\n",
    "    except AttributeError:\n",
    "        assert not (np.isnan(np.sum(outcome)) or np.isnan(np.sum(outcome)))\n",
    "    \n",
    "    # Get only the NONZERO elements\n",
    "    EPSILON = pow(10, -5)\n",
    "    idx = np.abs(outcome) > EPSILON\n",
    "    \n",
    "    # Extract those elements\n",
    "    outcome = np.array(outcome)[np.where(idx)]\n",
    "    predict = np.array(predict)[np.where(idx)]\n",
    "    \n",
    "    return np.mean(np.abs((outcome - predict) / outcome))\n",
    "\n",
    "score = mape(train_outcome, train_predict)\n",
    "print(\"Training MAPE: {:.6f}\".format(score))\n",
    "\n",
    "score = mape(valid_outcome, valid_predict)\n",
    "print(\"Validation MAPE: {:.6f}\".format(score))\n",
    "\n",
    "# The mean by cluster did #516 with 0.428684 on 2016-06-17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "<table class=\"table table-2\">\n",
    "        <tr>\n",
    "            <th>Data name</th>\n",
    "            <th>Data type</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>District ID</td>\n",
    "            <td>string</td>\n",
    "            <td>1,2,3,4 (the same as district mapping ID)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time slot</td>\n",
    "            <td>string</td>\n",
    "            <td>2016-01-23-1 (The first time slot on Jan. 23rd, 2016; one day is uniformly divided into 144 ten minute time slots)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Prediction value</td>\n",
    "            <td>double</td>\n",
    "            <td>6.0</td>\n",
    "        </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the date - timeslot column\n",
    "df_test_slots['datetimeslot'] = df_test_slots.date.dt.date.map(str) + '-' + df_test_slots.timeslot.astype(int).map(str)\n",
    "\n",
    "# Prepare output file\n",
    "cols = ['district_id', 'datetimeslot', 'gap_predict']\n",
    "df_test_slots[cols].to_csv('predict.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation with other result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Columns in order files\n",
    "cols = ['district_id', 'datetimeslot', 'gap_predict']\n",
    "\n",
    "# Open list of slots\n",
    "order_file = 'predict_final.csv'\n",
    "results_prior = pd.read_csv(order_file, sep = \",\", names = cols)\n",
    "\n",
    "# Compute correlation\n",
    "results = results_prior.merge(df_test_slots[cols], on = ['district_id', 'datetimeslot'])\n",
    "corr = results[['gap_predict_x', 'gap_predict_y']].corr()\n",
    "corr = corr.iloc[0,1]\n",
    "\n",
    "print(\"The correlation between the two results is {:.6f}\".format(corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
