{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of the data from the [Di-Tech Challenge](http://research.xiaojukeji.com/competition), organized by Didi Chuxing, a ride-hailing company in China. The data is described [here](http://research.xiaojukeji.com/competition/detail.action?competitionId=DiTech2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Warn about chained assignment?\n",
    "# pd.options.mode.chained_assignment = None\n",
    "\n",
    "from ggplot import *\n",
    "%matplotlib inline\n",
    "\n",
    "from time import clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Info Table\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>order_id</td>\n",
    "            <td>string</td>\n",
    "            <td>order ID</td>\n",
    "            <td>70fc7c2bd2caf386bb50f8fd5dfef0cf</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>driver_id</td>\n",
    "            <td>string</td>\n",
    "            <td>driver ID</td>\n",
    "            <td>56018323b921dd2c5444f98fb45509de</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>passenger_id</td>\n",
    "            <td>string</td>\n",
    "            <td>user ID</td>\n",
    "            <td>238de35f44bbe8a67bdea86a5b0f4719</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>start_district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>departure</td>\n",
    "            <td>d4ec2125aff74eded207d2d915ef682f</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>dest_district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>destination</td>\n",
    "            <td>929ec6c160e6f52c20a4217c7978f681</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Price</td>\n",
    "            <td>double</td>\n",
    "            <td>Price</td>\n",
    "            <td>37.5</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>string</td>\n",
    "            <td>Timestamp of the order</td>\n",
    "            <td>2016-01-15 00:35:11</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The Order Info Table shows the basic information of an order, including the passenger and the driver (if driver_id =NULL, it means the order was not answered by any driver), place of origin, destination, price and time. The fields order_id, driver_id, passenger_id, start_hash, and dest_hash are made not sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Columns in order files\n",
    "columns = ['order_id', 'driver_id', 'passenger_id', 'start_district_hash', 'dest_district_hash', 'price', 'time']\n",
    "\n",
    "# Open only one file\n",
    "# order_file_1 = \"data/season_1/training_data/order_data/order_data_2016-01-01\"\n",
    "# df = df_1 = pd.read_csv(order_file_1, sep = \"\\t\", names = columns, parse_dates = 'time')\n",
    "\n",
    "# Files are organized by dates\n",
    "n_files = 22\n",
    "order_files = [\"data/season_1/training_data/order_data/order_data_2016-01-{:02d}\".format(i) \n",
    "               for i in range(1, n_files)]\n",
    "\n",
    "# Open all of them\n",
    "order_dfs = []\n",
    "for order_file in order_files:\n",
    "    order_dfs.append(pd.read_csv(order_file, sep = \"\\t\", names = columns))\n",
    "df = pd.concat(order_dfs)\n",
    "\n",
    "# Recognize time column as time\n",
    "df['time'] = pd.to_datetime(df.time)\n",
    "# Set the row labels to the time stamp\n",
    "# df = df.set_index('time')\n",
    "\n",
    "# Keep first two weeks for training, next one week for validation.\n",
    "ind = df['time'] < pd.to_datetime('2016-01-15')\n",
    "df_train = df[ind]\n",
    "df_valid = df[~ind]\n",
    "\n",
    "# Keep a random number of the rows\n",
    "# df_train = df.sample(frac = 0.70, random_state = 111)\n",
    "# df_valid = df[~df.index.isin(df_train.index)]\n",
    "\n",
    "# Avoid looking at validation set during the exploration\n",
    "df = df_train\n",
    "\n",
    "# Quick look at the data frame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many entries? unique orders/passengers/drivers?\n",
    "\n",
    "num_entries = df.shape[0]\n",
    "print(\"{} entries\".format(num_entries))\n",
    "\n",
    "num_orders = len(df.order_id.unique())\n",
    "print(\"{} unique orders ({:.1%})\".format(num_orders, num_orders/num_entries))\n",
    "\n",
    "num_pass = len(df.passenger_id.unique())\n",
    "print(\"{} unique passengers ({:.1%})\".format(num_pass, num_pass/num_entries))\n",
    "      \n",
    "num_drivers = len(df.driver_id.unique())\n",
    "print(\"{} unique drivers ({:.1%})\".format(num_drivers, num_drivers/num_entries))\n",
    "\n",
    "num_start_district = len(df.start_district_hash.unique())\n",
    "print(\"{} unique starting districts ({:.1%})\".format(num_start_district, num_start_district/num_entries))\n",
    "\n",
    "num_dest_district = len(df.dest_district_hash.unique())\n",
    "print(\"{} unique destination districts ({:.1%})\".format(num_dest_district, num_dest_district/num_entries))\n",
    "\n",
    "# Price distribution\n",
    "print(\"\\n\")\n",
    "print(df.price.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the date, and implicitly make the time midnight.\n",
    "df['date'] = df.time.dt.date\n",
    "df['timeonly'] = df.time.dt.time\n",
    "\n",
    "# Compute time slot\n",
    "# One day is uniformly divided into 144 ten minute time slots. Indexed from 1 to 144.\n",
    "df['timeslot'] = (df['time'] - pd.to_datetime(df['date'])).astype('timedelta64[m]')//10 + 1\n",
    "\n",
    "# Drop the time column?\n",
    "# df = df.drop('time', axis = 1)\n",
    "\n",
    "# Is it a weekend or weekday?\n",
    "df['dow'] = df.time.dt.dayofweek\n",
    "df['weekend'] = df.dow >= 5\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Dates from {} to {}.\".format(df['time'].min(), df['time'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count how many rows per order_id and driver_id\n",
    "count = df[['order_id', 'driver_id']].groupby('order_id').count()\n",
    "count = count.reset_index()\n",
    "count = count['driver_id']\n",
    "\n",
    "# Orders picked up by more than one driver?\n",
    "print(sum(count > 1))\n",
    "# Yes..? Surprising.\n",
    "\n",
    "# Turns out there are duplicate and almost-duplicate entries. \n",
    "# The FAQ recommends just leaving them in.\n",
    "\n",
    "# Remove the duplicates\n",
    "# dup = df.duplicated(['order_id', 'driver_id', 'passenger_id', 'time'], keep = 'last')\n",
    "# df = df[~dup]\n",
    "\n",
    "# Proportion of orders not picked up by a driver\n",
    "s = sum(count == 0)\n",
    "l = len(count)\n",
    "\n",
    "print(\"There are {} orders-without-drivers out of {} orders: {:.1%}.\".format(s, l, s/l))\n",
    "# The gap is simply the number of orders not picked up.\n",
    "\n",
    "# FIXME This might be overcounting gap. Some driver_id still get 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Was order answered?\n",
    "df['is_gap'] = df['driver_id'].isnull()\n",
    "\n",
    "# Proportion of orders not picked up by a driver\n",
    "s = sum(df['is_gap'])\n",
    "l = len(df['is_gap'])\n",
    "print(\"There are {} orders-without-drivers out of {} orders: {:.1%}.\".format(s, l, s/l))\n",
    "\n",
    "# df.head(2)\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot of gap vs elapsed time in days group by ten minutes interval\n",
    "df['time_from_begin'] = (df['time'] - pd.to_datetime('2016-01-01')).astype('timedelta64[m]')//10/6/24\n",
    "\n",
    "cols = ['time_from_begin']\n",
    "df_select = df[cols + ['is_gap']]\n",
    "df_gap = df_select.groupby(cols).sum()\n",
    "df_gap = df_gap.reset_index()\n",
    "\n",
    "ggplot(aes('time_from_begin', 'is_gap'), data = df_gap) + \\\n",
    "    geom_point(color = 'gray') + \\\n",
    "    geom_line() + \\\n",
    "    xlab('Elapsed time in days') + ylab('Gap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute gap per time slot per district\n",
    "cols = ['start_district_hash', 'date', 'timeslot']\n",
    "\n",
    "# Number of missing drivers per group \n",
    "df['gap'] = df.groupby(cols)['is_gap'].transform('sum')\n",
    "\n",
    "# df_select = df[cols + ['is_gap']]\n",
    "# df_gap = df_select.groupby(cols).sum()\n",
    "\n",
    "# Flatten data frame after the group by\n",
    "# df_gap = df_gap.reset_index()\n",
    "# df_gap = df_gap.rename(columns = {'is_gap': 'gap'})\n",
    "\n",
    "# Sanity check: do the numbers add up?\n",
    "# print(sum(df_gap.gap))\n",
    "\n",
    "# Merge back into main data frame\n",
    "# df = df.merge(df_gap, on = cols, how = 'left')\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>string</td>\n",
    "            <td>Timestamp</td>\n",
    "            <td>2016-01-15 00:35:11</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Weather</td>\n",
    "            <td>int</td>\n",
    "            <td>Weather</td>\n",
    "            <td>7</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>temperature</td>\n",
    "            <td>double</td>\n",
    "            <td>Temperature</td>\n",
    "            <td>-9</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>PM2.5</td>\n",
    "            <td>double</td>\n",
    "            <td>pm25</td>\n",
    "            <td>66</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The Weather Info Table shows the weather info every 10 minutes each city. The weather field gives the weather conditions such as sunny, rainy, and snowy etc; all sensitive information has been removed. The unit of temperature is Celsius degree, and PM2.5 is the level of air pollutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Files are organized by dates\n",
    "n_files = 21\n",
    "weather_files = [\"data/season_1/training_data/weather_data/weather_data_2016-01-{:02d}\".format(i)\n",
    "                 for i in range(1, n_files)]\n",
    "\n",
    "# Open all of them\n",
    "columns = ['time', 'weather', 'temperature', 'pm25']\n",
    "weather_dfs = []\n",
    "for f in weather_files:\n",
    "    weather_dfs.append(pd.read_csv(f, sep = \"\\t\", names = columns))\n",
    "dfw = pd.concat(weather_dfs)\n",
    "\n",
    "# Extract date and time slot\n",
    "dfw['time'] = pd.to_datetime(dfw.time)\n",
    "dfw['date'] = dfw.time.dt.date\n",
    "dfw['timeslot'] = (dfw['time'] - pd.to_datetime(dfw['date'])).astype('timedelta64[m]')//10 + 1\n",
    "dfw = dfw.drop('time', axis = 1)\n",
    "\n",
    "# Merge into main data frame, and fill missing values\n",
    "# http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
    "df = df.merge(dfw, on = ['date', 'timeslot'], how = 'left')\n",
    "df.temperature = df.temperature.fillna(method = 'ffill') # forward fill\n",
    "# df.temperature = df.temperature.interpolate(method = 'time') # time-based interpolation\n",
    "\n",
    "# Quick peek\n",
    "dfw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rank\n",
    "df['start_district_count'] = df.groupby('start_district_hash')['start_district_hash'].transform('count')\n",
    "df['start_district_rank'] = df['start_district_count'].rank(ascending = False)\n",
    "df['district_rank'] = df['start_district_rank']\n",
    "\n",
    "# Number of most popular districts to keep\n",
    "k = 10\n",
    "\n",
    "num_entries = df.shape[0]\n",
    "df_filtered = df[df['start_district_rank'] < k]\n",
    "num_top = df_filtered['start_district_count'].sum()\n",
    "\n",
    "# districts_rank = df.start_district_hash.value_counts(sort = True)\n",
    "\n",
    "# Look at k top districts\n",
    "print(\"The first {} most popular districts account for {} out of {} ({:.1%})\".format(\n",
    "        k, num_top, num_entries, num_top/num_entries))\n",
    "\n",
    "# Extract most popular districts\n",
    "districts_top = df_filtered['start_district_hash']\n",
    "\n",
    "# Look only at most popular districts\n",
    "# df['district'] = np.nan\n",
    "# df.loc[df.start_district_hash.isin(districts_top), 'district'] = \\\n",
    "#     df.loc[df.start_district_hash.isin(districts_top), 'start_district_hash']\n",
    "    \n",
    "# One-hot encoding\n",
    "# dummies = pd.get_dummies(df['district'], dummy_na = False)\n",
    "# df = pd.concat((df.drop('district', axis = 1), dummies.astype(int)), axis = 1)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute time slot\n",
    "df_valid['time'] = pd.to_datetime(df_valid.time)\n",
    "df_valid['date'] = df_valid.time.dt.date\n",
    "df_valid['timeslot'] = (df_valid['time'] - pd.to_datetime(df_valid['date'])).astype('timedelta64[m]')//10 + 1\n",
    "\n",
    "# Day of week\n",
    "df_valid['dow'] = df_valid.time.dt.dayofweek\n",
    "df_valid['weekend'] = df_valid.dow >= 5\n",
    "\n",
    "# Compute gap per time slot per district\n",
    "df_valid['is_gap'] = df_valid['driver_id'].isnull()\n",
    "cols = ['start_district_hash', 'date', 'timeslot']\n",
    "df_select = df_valid[cols + ['is_gap']]\n",
    "df_gap = df_select.groupby(cols).sum()\n",
    "df_gap = df_gap.reset_index().rename(columns = {'is_gap': 'gap'})\n",
    "df_valid = df_valid.merge(df_gap, on = cols, how = 'left')\n",
    "\n",
    "# Merge temperature\n",
    "df_valid = df_valid.merge(dfw, on = ['date', 'timeslot'], how = 'left')\n",
    "df_valid.temperature = df_valid.temperature.fillna(method = 'ffill')\n",
    "\n",
    "# One-hot encoding of districts\n",
    "# df_valid['district'] = np.nan\n",
    "# df_valid.loc[df_valid.start_district_hash.isin(districts), 'district'] = \\\n",
    "#     df_valid.loc[df_valid.start_district_hash.isin(districts), 'start_district_hash']\n",
    "# dummies = pd.get_dummies(df_valid['district'], dummy_na = False)\n",
    "# df_valid = pd.concat((df_valid.drop('district', axis = 1), dummies.astype(int)), axis = 1)\n",
    "\n",
    "# Replace district by popularity\n",
    "districts_rank = df[['start_district_hash', 'district_rank']]\n",
    "districts_rank = districts_rank.drop_duplicates(subset = ['start_district_hash'], keep = 'first')\n",
    "districts_rank = districts_rank.set_index('start_district_hash')['district_rank']\n",
    "df_valid['start_district_rank'] = districts_rank[df_valid.start_district_hash].reset_index()['district_rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions by Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make first prediction by simply taking the mean per start_district_hash per timeslot per weekend\n",
    "cols = ['start_district_hash', 'weekend', 'timeslot']\n",
    "# cols = ['start_district_hash', 'dow', 'timeslot']\n",
    "\n",
    "df_select = df[cols + ['gap']]\n",
    "gap_cluster = df_select.groupby(cols).min().reset_index()\n",
    "# gap_cluster = df_select.groupby(cols).mean().reset_index()\n",
    "gap_cluster = gap_cluster.rename(columns = {'gap': 'gap_cluster'})\n",
    "\n",
    "# Work directly on main data frame\n",
    "# df['gap_cluster'] = df[cols + ['gap']].groupby(cols)['gap'].transform('min')\n",
    "\n",
    "# Merge back into main data frame\n",
    "df_cluster = df.merge(gap_cluster, on = cols, how = 'left')\n",
    "\n",
    "# Quick look\n",
    "df.head(2)\n",
    "\n",
    "# Set given outcome and predictions\n",
    "train_outcome = df_cluster['gap']\n",
    "train_predict = df_cluster['gap_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "df_valid_cluster = df_valid[cols + ['gap']].merge(gap_cluster, on = cols, how = 'left')\n",
    "\n",
    "valid_outcome = df_valid_cluster['gap']\n",
    "valid_predict = df_valid_cluster['gap_cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select features\n",
    "cols = ['start_district_rank', 'dow', 'timeslot', 'temperature']\n",
    "# cols = ['start_district_rank', 'weekend', 'timeslot', 'temperature']\n",
    "train = df[cols]\n",
    "\n",
    "# Select regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor(n_estimators = 10)\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# reg = DecisionTreeRegressor(max_depth = 3)\n",
    "\n",
    "# Fit training data\n",
    "start = clock()\n",
    "reg.fit(train, train_outcome)\n",
    "print(\"Fit in {:.0f} seconds.\".format(clock() - start))\n",
    "\n",
    "# Extrapolate to test data\n",
    "start = clock()\n",
    "train_predict = reg.predict(train)\n",
    "print(\"Extrapolate in {:.0f} seconds.\".format(clock() - start))\n",
    "\n",
    "# Reference outcome\n",
    "train_outcome = df['gap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "valid_outcome = df_valid['gap']\n",
    "valid_predict = reg.predict(df_valid[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Consider di districts and tj time slots, and the supply-demand gap gapij , and your prediction is sij, we use as the evaluation metrics: \n",
    "![MAPE](figures/mape.jpg)\n",
    "The lowest MAPE will be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(outcome, predict):\n",
    "    # Compute MAPE score. Lower is better.\n",
    "    import numpy as np\n",
    "    \n",
    "    # Compute errors summand for summand with nonzero denominator\n",
    "    diff = (outcome - predict) / outcome.replace({0: np.nan})\n",
    "    diff = diff.replace({np.nan: 0})\n",
    "    diff = diff.abs()\n",
    "    \n",
    "    # Compute the average over all district and timeslots for which outcome is NONZERO\n",
    "    nq = len(outcome.nonzero()[0])\n",
    "    return diff.sum() / nq\n",
    "\n",
    "# As of June 7th...\n",
    "# lowest score online is 0.224257,\n",
    "# 100th is 0.27747,\n",
    "# 500th is 0.360159.\n",
    "\n",
    "score = mape(train_outcome, train_predict)\n",
    "print(\"Training MAPE: {:.6f}\".format(score))\n",
    "\n",
    "# MAPE on training set\n",
    "# ['start_district_hash', 'weekend', 'timeslot', 'gap_cluster'] -- 0.003352\n",
    "# ['start_district_hash', 'dow', 'timeslot', 'gap_cluster'] -- 0.011473\n",
    "# ['start_district_hash', 'dow', 'timeslot', 'ffill-temperature', DTR] -- 0.777944\n",
    "# ['start_district_hash', 'dow', 'timeslot', 'ffill-temperature', RFR] -- 0.566997\n",
    "\n",
    "score = mape(valid_outcome, valid_predict)\n",
    "print(\"Validation MAPE: {:.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# District Info Table\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>District hash</td>\n",
    "            <td>90c5a34f06ac86aee0fd70e2adce7d8a</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>district_id</td>\n",
    "            <td>string</td>\n",
    "            <td>District ID</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The District Info Table shows the information about the districts to be evaluated in the contest. You need to do the prediction given the districts from the District Definition Table. In the submission of the results, you need to map the district hash value to district mapped ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the starting district_hash as the associated disctrict\n",
    "df['district_hash'] = df['start_district_hash']\n",
    "\n",
    "# Load district conversion table\n",
    "district_file = 'data/season_1/training_data/cluster_map/cluster_map'\n",
    "district = pd.read_csv(district_file, sep = '\\t', names = ['district_hash', 'district_id'])\n",
    "\n",
    "# How many districts?\n",
    "print(district.shape)\n",
    "\n",
    "# Replace district_hash by district_id in data frame\n",
    "df = df.merge(district, on = 'district_hash', how = 'left')\n",
    "# df = df.drop('district_hash', axis = 1)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "<table class=\"table table-2\">\n",
    "        <tr>\n",
    "            <th>Data name</th>\n",
    "            <th>Data type</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>District ID</td>\n",
    "            <td>string</td>\n",
    "            <td>1,2,3,4 (the same as district mapping ID)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time slot</td>\n",
    "            <td>string</td>\n",
    "            <td>2016-01-23-1 (The first time slot on Jan. 23rd, 2016; one day is uniformly divided into 144 ten minute time slots)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Prediction value</td>\n",
    "            <td>double</td>\n",
    "            <td>6.0</td>\n",
    "        </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the date - timeslot column\n",
    "df['datetimeslot'] = df.date.map(str) + '-' + df.timeslot.astype(int).map(str)\n",
    "\n",
    "# Prepare output file\n",
    "cols = ['district_id', 'datetimeslot']\n",
    "final = df[cols + ['gap_cluster']].groupby(cols).mean().reset_index()\n",
    "final.to_csv(\"predict.csv\", index = False, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
