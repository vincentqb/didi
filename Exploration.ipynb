{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of the data from the [Di-Tech Challenge](http://research.xiaojukeji.com/competition), organized by Didi Chuxing, a ride-hailing company in China. The data is described [here](http://research.xiaojukeji.com/competition/detail.action?competitionId=DiTech2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import clock\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Warn about chained assignment in pandas?\n",
    "# pd.options.mode.chained_assignment = None\n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "from ggplot import *\n",
    "%matplotlib inline\n",
    "\n",
    "# path_to_test = 'data/season_1/test_set_1/'\n",
    "path_to_test = 'data/season_1/test_set_2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Info Table\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>order_id</td>\n",
    "            <td>string</td>\n",
    "            <td>order ID</td>\n",
    "            <td>70fc7c2bd2caf386bb50f8fd5dfef0cf</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>driver_id</td>\n",
    "            <td>string</td>\n",
    "            <td>driver ID</td>\n",
    "            <td>56018323b921dd2c5444f98fb45509de</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>passenger_id</td>\n",
    "            <td>string</td>\n",
    "            <td>user ID</td>\n",
    "            <td>238de35f44bbe8a67bdea86a5b0f4719</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>start_district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>departure</td>\n",
    "            <td>d4ec2125aff74eded207d2d915ef682f</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>dest_district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>destination</td>\n",
    "            <td>929ec6c160e6f52c20a4217c7978f681</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Price</td>\n",
    "            <td>double</td>\n",
    "            <td>Price</td>\n",
    "            <td>37.5</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>string</td>\n",
    "            <td>Timestamp of the order</td>\n",
    "            <td>2016-01-15 00:35:11</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The Order Info Table shows the basic information of an order, including the passenger and the driver (if driver_id =NULL, it means the order was not answered by any driver), place of origin, destination, price and time. The fields order_id, driver_id, passenger_id, start_hash, and dest_hash are made not sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_orders(order_files):\n",
    "    \"\"\"\n",
    "    Create data frames for order data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Columns in order files\n",
    "    columns = ['order_id', 'driver_id', 'passenger_id', \n",
    "               'start_district_hash', 'dest_district_hash', 'price', 'time']\n",
    "\n",
    "    # Open all of them\n",
    "    order_dfs = []\n",
    "    for order_file in order_files:\n",
    "        order_dfs.append(pd.read_csv(order_file, sep = \"\\t\", names = columns))\n",
    "    df = pd.concat(order_dfs)\n",
    "\n",
    "    # Recognize time column as time\n",
    "    df['time'] = pd.to_datetime(df.time)\n",
    "    # Set the row labels to the time stamp\n",
    "    # df = df.set_index('time')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Files are organized by dates\n",
    "order_files = [\"data/season_1/training_data/order_data/order_data_2016-01-{:02d}\".format(i) \n",
    "#                for i in range(1, 22)]\n",
    "               for i in range(10, 16)]\n",
    "\n",
    "df = load_orders(order_files)\n",
    "\n",
    "# Keep first two weeks for training, next one week for validation.\n",
    "ind = df['time'] < pd.to_datetime('2016-01-15')\n",
    "df_train = df[ind]\n",
    "df_valid = df[~ind]\n",
    "\n",
    "# Keep a random number of the rows\n",
    "# df_train = df.sample(frac = 0.70, random_state = 111)\n",
    "# df_valid = df[~df.index.isin(df_train.index)]\n",
    "\n",
    "# Avoid looking at validation set during the exploration\n",
    "df = df_train\n",
    "\n",
    "# Quick look at the data frame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dates?\n",
    "\n",
    "print(\"Dates from {} to {}.\".format(df['time'].min(), df['time'].max()))\n",
    "\n",
    "# How many entries? unique orders/passengers/drivers?\n",
    "\n",
    "num_entries = df.shape[0]\n",
    "print(\"{} entries\".format(num_entries))\n",
    "\n",
    "num_orders = len(df.order_id.unique())\n",
    "print(\"{} unique orders ({:.1%})\".format(num_orders, num_orders/num_entries))\n",
    "\n",
    "num_pass = len(df.passenger_id.unique())\n",
    "print(\"{} unique passengers ({:.1%})\".format(num_pass, num_pass/num_entries))\n",
    "      \n",
    "num_drivers = len(df.driver_id.unique())\n",
    "print(\"{} unique drivers ({:.1%})\".format(num_drivers, num_drivers/num_entries))\n",
    "\n",
    "num_start_district = len(df.start_district_hash.unique())\n",
    "print(\"{} unique starting districts ({:.1%})\".format(num_start_district, num_start_district/num_entries))\n",
    "\n",
    "num_dest_district = len(df.dest_district_hash.unique())\n",
    "print(\"{} unique destination districts ({:.1%})\".format(num_dest_district, num_dest_district/num_entries))\n",
    "\n",
    "# Price distribution\n",
    "\n",
    "print(\"\\n\")\n",
    "print(df.price.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Attach time-related info \n",
    "\n",
    "def compute_timeinfo(df, dateonly = False):\n",
    "    # Input: data frame with time (or date) column.\n",
    "    \n",
    "    if dateonly:\n",
    "        # Determine day of week, weekend.\n",
    "        df['dow'] = pd.to_datetime(df.date).dt.dayofweek\n",
    "        df['weekend'] = df.dow >= 5\n",
    "    else:\n",
    "        # Extract the date, and implicitly make the time midnight.\n",
    "        df['date'] = df.time.dt.date\n",
    "        df['timeonly'] = df.time.dt.time\n",
    "        \n",
    "        # Compute 10-min timeslots -- absolute and per day.\n",
    "        df['timeslot_absolute'] = (df['time'] - pd.to_datetime('2016-01-01')).astype('timedelta64[m]')//10\n",
    "        df['timeslot_day'] = df['timeslot_absolute'] % (24 * 6)\n",
    "        df['timeslot'] = df['timeslot_day'] % (24*6) + 1\n",
    "    \n",
    "        # Determine day of week, weekend.\n",
    "        df['dow'] = df.time.dt.dayofweek\n",
    "        df['weekend'] = df.dow >= 5\n",
    "    \n",
    "        # Drop the time column?\n",
    "        # df = df.drop('time', axis = 1)\n",
    "    \n",
    "compute_timeinfo(df)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot of gap vs elapsed time in days group by ten minutes interval\n",
    "\n",
    "# Convert to days\n",
    "df['time_from_begin'] = df['timeslot_absolute']/6/24\n",
    "\n",
    "# Count how many rows per timeslot\n",
    "# count = df[['order_id', 'driver_id']].groupby('order_id').count()\n",
    "count = df.groupby('time_from_begin')['time_from_begin'].count()\n",
    "count = pd.DataFrame(count)\n",
    "count.columns = ['count']\n",
    "count.reset_index(level=0, inplace=True)\n",
    "\n",
    "ggplot(aes('time_from_begin', 'count'), data = count) + \\\n",
    "    geom_point(color = 'gray') + \\\n",
    "    geom_line() + \\\n",
    "    xlab('Elapsed time in days') + ylab('Order count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count how many rows per order_id and driver_id\n",
    "count = df[['order_id', 'driver_id']].groupby('order_id').count()\n",
    "count = count.reset_index()\n",
    "count = count['driver_id']\n",
    "\n",
    "# Orders picked up by more than one driver?\n",
    "print(\"Number of orders taken by more than one driver: {}\".format(sum(count > 1)))\n",
    "# Yes..? Surprising.\n",
    "\n",
    "# Turns out there are duplicate and almost-duplicate entries. \n",
    "# The FAQ recommends just leaving them in.\n",
    "\n",
    "# Remove the duplicates\n",
    "# dup = df.duplicated(['order_id', 'driver_id', 'passenger_id', 'time'], keep = 'last')\n",
    "# df = df[~dup]\n",
    "\n",
    "# Proportion of orders not picked up by a driver\n",
    "s = sum(count == 0)\n",
    "l = len(count)\n",
    "\n",
    "print(\"There are {} orders-without-drivers out of {} orders: {:.1%}.\".format(s, l, s/l))\n",
    "# The gap is simply the number of orders not picked up.\n",
    "\n",
    "# FIXME This might be overcounting gap. Some driver_id still get 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Was order answered?\n",
    "df['is_gap'] = df['driver_id'].isnull()\n",
    "\n",
    "# Proportion of orders not picked up by a driver\n",
    "s = sum(df['is_gap'])\n",
    "l = len(df['is_gap'])\n",
    "print(\"There are {} orders-without-drivers out of {} orders: {:.1%}.\".format(s, l, s/l))\n",
    "\n",
    "# df.head(2)\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot of gap vs elapsed time in days group by ten minutes interval\n",
    "\n",
    "cols = ['time_from_begin']\n",
    "df_select = df[cols + ['is_gap']]\n",
    "df_gap = df_select.groupby(cols).sum()\n",
    "df_gap = df_gap.reset_index()\n",
    "\n",
    "ggplot(aes('time_from_begin', 'is_gap'), data = df_gap) + \\\n",
    "    geom_point(color = 'gray') + \\\n",
    "    geom_line() + \\\n",
    "    xlab('Elapsed time in days') + ylab('Gap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute gap per timeslot per district\n",
    "# cols = ['start_district_hash', 'date', 'timeslot']\n",
    "\n",
    "# Number of missing drivers per group \n",
    "# df['gap'] = df.groupby(cols)['is_gap'].transform('sum')\n",
    "\n",
    "# df.head(2)\n",
    "\n",
    "# Make new data frame with gap per timeslot per district\n",
    "\n",
    "def compute_gap(df):\n",
    "    \n",
    "    # Gap is when no driver\n",
    "    df['is_gap'] = df['driver_id'].isnull()\n",
    "    \n",
    "    # Aggregate\n",
    "    cols = ['start_district_hash', 'date', 'timeslot']\n",
    "    df_select = df[cols + ['is_gap']]\n",
    "    df_gap = df_select.groupby(cols).sum()\n",
    "\n",
    "    # Flatten data frame\n",
    "    df_gap = df_gap.reset_index()\n",
    "    df_gap = df_gap.rename(columns = {'is_gap': 'gap'})\n",
    "    \n",
    "    return df_gap\n",
    "\n",
    "df_gap = compute_gap(df)\n",
    "\n",
    "# Sanity check: do the number add up?\n",
    "print(sum(df_gap.gap))\n",
    "\n",
    "# Merge back into main data frame\n",
    "# df = df.merge(df_gap, on = cols, how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>string</td>\n",
    "            <td>Timestamp</td>\n",
    "            <td>2016-01-15 00:35:11</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Weather</td>\n",
    "            <td>int</td>\n",
    "            <td>Weather</td>\n",
    "            <td>7</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>temperature</td>\n",
    "            <td>double</td>\n",
    "            <td>Temperature</td>\n",
    "            <td>-9</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>PM2.5</td>\n",
    "            <td>double</td>\n",
    "            <td>pm25</td>\n",
    "            <td>66</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The Weather Info Table shows the weather info every 10 minutes each city. The weather field gives the weather conditions such as sunny, rainy, and snowy etc; all sensitive information has been removed. The unit of temperature is Celsius degree, and PM2.5 is the level of air pollutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_merge_weather(weather_files, df):\n",
    "\n",
    "    # Open all of them\n",
    "    columns = ['time', 'weather', 'temperature', 'pm25']\n",
    "    weather_dfs = []\n",
    "    for f in weather_files:\n",
    "        weather_dfs.append(pd.read_csv(f, sep = \"\\t\", names = columns))\n",
    "    dfw = pd.concat(weather_dfs)\n",
    "\n",
    "    # Extract date and time slot\n",
    "    dfw['time'] = pd.to_datetime(dfw.time)\n",
    "    dfw['date'] = dfw.time.dt.date\n",
    "    dfw['timeslot'] = (dfw['time'] - pd.to_datetime(dfw['date'])).astype('timedelta64[m]')//10 + 1\n",
    "    dfw = dfw.drop('time', axis = 1)\n",
    "    \n",
    "    # Merge into main data frame, and fill missing values\n",
    "    # http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
    "    df = df.merge(dfw, on = ['date', 'timeslot'], how = 'left')\n",
    "    df.temperature = df.temperature.fillna(method = 'ffill') # forward fill\n",
    "    df.temperature = df.temperature.fillna(method = 'bfill') # backward fill\n",
    "    # df.temperature = df.temperature.interpolate(method = 'time') # time-based interpolation\n",
    "\n",
    "    return df, dfw\n",
    "    \n",
    "\n",
    "# Files are organized by dates\n",
    "weather_files = glob.glob('data/season_1/training_data/weather_data/*')\n",
    "# weather_files = [\"data/season_1/training_data/weather_data/weather_data_2016-01-{:02d}\".format(i)\n",
    "#                  for i in range(1, 21)]\n",
    "\n",
    "df, dfw = load_merge_weather(weather_files, df)\n",
    "\n",
    "# Quick peek\n",
    "# df, dfw = load_merge_weather(weather_files, df)\n",
    "# dfw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge into grouped data frame\n",
    "df_gap = df_gap.merge(dfw, on = ['date', 'timeslot'], how = 'left')\n",
    "df_gap.temperature = df_gap.temperature.fillna(method = 'ffill')\n",
    "df_gap.temperature = df_gap.temperature.fillna(method = 'bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ranking districts\n",
    "districts_rank = df.start_district_hash.value_counts(sort = True)\n",
    "districts_rank = districts_rank.rank(ascending = False, method = 'dense')\n",
    "districts_rank.head(5)\n",
    "\n",
    "# Create conversion table\n",
    "districts_convert = pd.DataFrame(data = districts_rank)\n",
    "districts_convert.reset_index(level=0, inplace = True)\n",
    "districts_convert.columns = ['start_district_hash', 'start_district_rank']\n",
    "# districts_convert.head(2)\n",
    "\n",
    "# Replace hash by rank\n",
    "df_gap = df_gap.merge(districts_convert, on = 'start_district_hash', how = 'left')\n",
    "# df_gap = df_gap.drop('start_district_hash', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gap.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rank\n",
    "df['start_district_count'] = df.groupby('start_district_hash')['start_district_hash'].transform('count')\n",
    "df['start_district_rank'] = df['start_district_count'].rank(ascending = False, method = 'dense')\n",
    "df['district_rank'] = df['start_district_rank']\n",
    "\n",
    "# Number of most popular districts to keep\n",
    "k = 10\n",
    "\n",
    "num_entries = df.shape[0]\n",
    "df_filtered = df[df['start_district_rank'] <= k]\n",
    "num_top = df_filtered.groupby('start_district_hash')['start_district_count'].mean().sum()\n",
    "\n",
    "# Look at k top districts\n",
    "print(\"The first {} most popular districts account for {} out of {} ({:.1%})\".format(\n",
    "        k, num_top, num_entries, num_top/num_entries))\n",
    "\n",
    "# Extract most popular districts\n",
    "districts_top = df_filtered['start_district_hash']\n",
    "\n",
    "# Look only at most popular districts\n",
    "# df['district'] = np.nan\n",
    "# df.loc[df.start_district_hash.isin(districts_top), 'district'] = \\\n",
    "#     df.loc[df.start_district_hash.isin(districts_top), 'start_district_hash']\n",
    "    \n",
    "# One-hot encoding\n",
    "# dummies = pd.get_dummies(df['district'], dummy_na = False)\n",
    "# df = pd.concat((df.drop('district', axis = 1), dummies.astype(int)), axis = 1)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute timeslot\n",
    "compute_timeinfo(df_valid)\n",
    "\n",
    "# Compute gap per time slot per district\n",
    "df_valid_gap = compute_gap(df_valid)\n",
    "\n",
    "# Merge temperature\n",
    "df_valid_gap = df_valid_gap.merge(dfw, on = ['date', 'timeslot'], how = 'left')\n",
    "df_valid_gap.temperature = df_valid_gap.temperature.fillna(method = 'ffill')\n",
    "df_valid_gap.temperature = df_valid_gap.temperature.fillna(method = 'bfill')\n",
    "\n",
    "# One-hot encoding of districts\n",
    "# df_valid['district'] = np.nan\n",
    "# df_valid.loc[df_valid.start_district_hash.isin(districts), 'district'] = \\\n",
    "#     df_valid.loc[df_valid.start_district_hash.isin(districts), 'start_district_hash']\n",
    "# dummies = pd.get_dummies(df_valid['district'], dummy_na = False)\n",
    "# df_valid = pd.concat((df_valid.drop('district', axis = 1), dummies.astype(int)), axis = 1)\n",
    "\n",
    "# Replace district by popularity\n",
    "df_valid_gap = df_valid_gap.merge(districts_convert, on = 'start_district_hash', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Columns in order files\n",
    "columns = ['datetimeslot']\n",
    "\n",
    "# Open list of slots\n",
    "# order_file = path_to_test + 'read_me_1.txt'\n",
    "order_file = path_to_test + 'read_me_2.txt'\n",
    "df_test_slots = pd.read_csv(order_file, sep = \"\\t\", names = columns, skiprows = 1)\n",
    "\n",
    "# Extract date and timeslot [0,143]\n",
    "df_test_slots['date'] = pd.to_datetime(df_test_slots.datetimeslot.str[:10])\n",
    "df_test_slots['timeslot'] = df_test_slots.datetimeslot.str[11:]\n",
    "df_test_slots['timeslot'] = df_test_slots.timeslot.astype(int)\n",
    "# df_test.dtypes\n",
    "\n",
    "# Day of week\n",
    "compute_timeinfo(df_test_slots, dateonly = True)\n",
    "\n",
    "# df_test_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract timeslots for testing in validation\n",
    "timeslots = df_test_slots.timeslot.unique()\n",
    "df_valid_gap_selected = df_valid_gap[df_valid_gap['timeslot'].isin(timeslots)]\n",
    "\n",
    "df_valid_gap = df_valid_gap_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load test order files\n",
    "order_files = glob.glob(path_to_test + 'order_data/*')\n",
    "# order_files = [path_to_test + \"order_data/order_data_2016-01-{:02d}_test\".format(i) \n",
    "#                for i in [22, 24, 26, 28, 30]]\n",
    "\n",
    "df_test = load_orders(order_files)\n",
    "\n",
    "# Attach time info\n",
    "compute_timeinfo(df_test)\n",
    "\n",
    "# Compute gap per time slot per district\n",
    "df_test_gap = compute_gap(df_test)\n",
    "\n",
    "# Attach district rank\n",
    "df_test_gap = df_test_gap.merge(districts_convert, on = 'start_district_hash', how = 'left')\n",
    "\n",
    "# Load and merge weather data\n",
    "order_files = glob.glob(path_to_test + 'weather_data/*')\n",
    "# weather_files = [path_to_test + \"weather_data/weather_data_2016-01-{:02d}_test\".format(i)\n",
    "#                  for i in [22, 24, 26, 28, 30]]\n",
    "\n",
    "df_test_gap, _ = load_merge_weather(weather_files, df_test_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all data frames for final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is it a weekend or weekday?\n",
    "compute_timeinfo(df_gap, dateonly = True)\n",
    "compute_timeinfo(df_valid_gap, dateonly = True)\n",
    "compute_timeinfo(df_test_gap, dateonly = True)\n",
    "\n",
    "# Concatenate\n",
    "df_all_gap = pd.concat([df_gap, df_valid_gap, df_test_gap])\n",
    "\n",
    "# Ready for final training?\n",
    "# df_gap = df_all_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions by Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make first prediction by simply taking the mean per start_district_hash per timeslot per weekend\n",
    "cols = ['start_district_hash', 'weekend', 'timeslot']\n",
    "# cols = ['start_district_hash', 'dow', 'timeslot']\n",
    "\n",
    "# Make prediction table per group\n",
    "df_select = df_gap[cols + ['gap']]\n",
    "df_grouped = df_select.groupby(cols)\n",
    "gap_cluster = df_grouped.median() # median, mean, min, max\n",
    "gap_cluster = gap_cluster.reset_index().rename(columns = {'gap': 'gap_cluster'})\n",
    "\n",
    "# Merge back into main data frame\n",
    "df_cluster = df_gap.merge(gap_cluster, on = cols, how = 'left')\n",
    "\n",
    "# Set given outcome and predictions\n",
    "train_outcome = df_cluster['gap']\n",
    "train_predict = df_cluster['gap_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge gap_cluster in validation set\n",
    "df_valid_cluster = df_valid_gap[cols + ['gap']].merge(gap_cluster, on = cols, how = 'left')\n",
    "\n",
    "# Set given outcome and predictions\n",
    "valid_outcome = df_valid_cluster['gap']\n",
    "# FIXME Some values are missing predictions!\n",
    "df_valid_cluster['gap_cluster'] = df_valid_cluster['gap_cluster'].fillna(method = 'ffill')\n",
    "df_valid_cluster['gap_cluster'] = df_valid_cluster['gap_cluster'].fillna(method = 'bfill')\n",
    "valid_predict = df_valid_cluster['gap_cluster']\n",
    "\n",
    "# Merge gap_cluster in test set\n",
    "df_test = df_test.merge(gap_cluster, on = cols, how = 'left')\n",
    "\n",
    "# Set predictions\n",
    "# FIXME Some values are missing predictions!\n",
    "df_test['gap_cluster'] = df_test['gap_cluster'].fillna(method = 'ffill')\n",
    "df_test['gap_cluster'] = df_test['gap_cluster'].fillna(method = 'bfill')\n",
    "test_predict = df_test['gap_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make prediction table per group with ALL the data -- for final submission\n",
    "df_all_select = df_all_gap[cols + ['gap']]\n",
    "df_all_grouped = df_all_select.groupby(cols)\n",
    "df_all_cluster = df_all_grouped.median() # median, mean, min, max\n",
    "df_all_cluster = df_all_cluster.reset_index().rename(columns = {'gap': 'gap_cluster'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select features\n",
    "cols = ['start_district_rank', 'dow', 'timeslot', 'temperature']\n",
    "# cols = ['start_district_rank', 'weekend', 'timeslot', 'temperature']\n",
    "train = df_gap[cols]\n",
    "\n",
    "# Reference outcome\n",
    "train_outcome = df_gap['gap']\n",
    "\n",
    "# Select regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor(n_estimators = 10)\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# reg = DecisionTreeRegressor(max_depth = 3)\n",
    "\n",
    "# Fit training data\n",
    "start = clock()\n",
    "reg.fit(train, train_outcome)\n",
    "print(\"Fit in {:.0f} seconds.\".format(clock() - start))\n",
    "\n",
    "# Extrapolate to test data\n",
    "start = clock()\n",
    "# train_predict = reg.predict(train)\n",
    "print(\"Extrapolate in {:.0f} seconds.\".format(clock() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "# valid_outcome = df_valid_gap['gap']\n",
    "# valid_predict = reg.predict(df_valid_gap[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Consider di districts and tj time slots, and the supply-demand gap gapij , and your prediction is sij, we use as the evaluation metrics: \n",
    "![MAPE](figures/mape.jpg)\n",
    "The lowest MAPE will be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(outcome, predict):\n",
    "    \"\"\"\n",
    "    Compute MAPE score. Lower is better.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verify that there are no missing values\n",
    "    assert not (outcome.isnull().values.any() or predict.isnull().values.any())\n",
    "    \n",
    "    \n",
    "    # Compute errors summand for summand with nonzero denominator\n",
    "    diff = (outcome - predict) / outcome.replace({0: np.nan})\n",
    "    diff = diff.replace({np.nan: 0})\n",
    "    diff = diff.abs()\n",
    "    \n",
    "    # Compute the average over all district and timeslots for which outcome is NONZERO\n",
    "    nq = len(outcome.nonzero()[0])\n",
    "    return diff.sum() / nq\n",
    "\n",
    "# As of June 7th...\n",
    "# lowest score online is 0.224257,\n",
    "# 100th is 0.27747,\n",
    "# 500th is 0.360159.\n",
    "\n",
    "score = mape(train_outcome, train_predict)\n",
    "print(\"Training MAPE: {:.6f}\".format(score))\n",
    "\n",
    "score = mape(valid_outcome, valid_predict)\n",
    "print(\"Validation MAPE: {:.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(outcome, predict):\n",
    "    \"\"\"\n",
    "    Compute MAPE score. Lower is better.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verify that there are no missing values\n",
    "    assert not (outcome.isnull().values.any() or predict.isnull().values.any())\n",
    "    # assert not (np.isnan(np.sum(outcome)) or np.isnan(np.sum(outcome)))\n",
    "    \n",
    "    # Get only the nonzero elements\n",
    "    EPSILON = pow(10, -5)\n",
    "    idx = np.abs(outcome) < EPSILON\n",
    "    \n",
    "    return np.mean(np.abs((outcome[idx] - predict[idx]) / outcome[idx]))\n",
    "\n",
    "score = mape(train_outcome, train_predict)\n",
    "print(\"Training MAPE: {:.6f}\".format(score))\n",
    "\n",
    "score = mape(valid_outcome, valid_predict)\n",
    "print(\"Validation MAPE: {:.6f}\".format(score))\n",
    "\n",
    "# As of June 7th...\n",
    "# lowest score online is 0.224257,\n",
    "# 100th is 0.27747,\n",
    "# 500th is 0.360159."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# District Info Table\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>District hash</td>\n",
    "            <td>90c5a34f06ac86aee0fd70e2adce7d8a</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>district_id</td>\n",
    "            <td>string</td>\n",
    "            <td>District ID</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The District Info Table shows the information about the districts to be evaluated in the contest. You need to do the prediction given the districts from the District Definition Table. In the submission of the results, you need to map the district hash value to district mapped ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the starting district_hash as the associated disctrict\n",
    "col = 'start_district_hash'\n",
    "\n",
    "# Load district conversion table\n",
    "district_file = 'data/season_1/training_data/cluster_map/cluster_map'\n",
    "district = pd.read_csv(district_file, sep = '\\t', names = [col, 'district_id'])\n",
    "\n",
    "# How many districts?\n",
    "print(\"There are {} districts in the district file.\".format(district.shape[0]))\n",
    "\n",
    "# Replace district_hash by district_id in data frame\n",
    "df_test = df_test.merge(district, on = col, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create all combination of districts and time slots\n",
    "df_test_slots['key'] = 1\n",
    "district['key'] = 1\n",
    "df_test_slots = df_test_slots.merge(district, on = 'key')\n",
    "df_test_slots = df_test_slots.drop('key', axis = 1)\n",
    "\n",
    "df_test_slots.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set predictions\n",
    "cols = ['start_district_hash', 'weekend', 'timeslot']\n",
    "\n",
    "df_test_slots = df_test_slots.merge(gap_cluster, on = cols, how = 'left')\n",
    "# df_test_slots = df_test_slots.merge(gap_all_cluster, on = cols, how = 'left') # ..for final submission!\n",
    "\n",
    "# FIXME Some values are missing predictions!\n",
    "df_test_slots['gap_cluster'] = df_test_slots['gap_cluster'].fillna(method = 'ffill')\n",
    "df_test_slots['gap_cluster'] = df_test_slots['gap_cluster'].fillna(method = 'bfill')\n",
    "\n",
    "# Relabel\n",
    "df_test_slots['gap_predict'] = df_test_slots['gap_cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "<table class=\"table table-2\">\n",
    "        <tr>\n",
    "            <th>Data name</th>\n",
    "            <th>Data type</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>District ID</td>\n",
    "            <td>string</td>\n",
    "            <td>1,2,3,4 (the same as district mapping ID)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time slot</td>\n",
    "            <td>string</td>\n",
    "            <td>2016-01-23-1 (The first time slot on Jan. 23rd, 2016; one day is uniformly divided into 144 ten minute time slots)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Prediction value</td>\n",
    "            <td>double</td>\n",
    "            <td>6.0</td>\n",
    "        </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the date - timeslot column\n",
    "df_test_slots['datetimeslot'] = df_test_slots.date.dt.date.map(str) + '-' + df_test_slots.timeslot.astype(int).map(str)\n",
    "\n",
    "# Prepare output file\n",
    "cols = ['district_id', 'datetimeslot', 'gap_predict']\n",
    "df_test_slots[cols].to_csv('predict.csv', index = False, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
