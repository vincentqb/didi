{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of the data from the [Di-Tech Challenge](http://research.xiaojukeji.com/competition), organized by Didi Chuxing, a ride-hailing company in China. The data is described [here](http://research.xiaojukeji.com/competition/detail.action?competitionId=DiTech2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import clock\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Warn about chained assignment in pandas?\n",
    "# pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Info Table\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>order_id</td>\n",
    "            <td>string</td>\n",
    "            <td>order ID</td>\n",
    "            <td>70fc7c2bd2caf386bb50f8fd5dfef0cf</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>driver_id</td>\n",
    "            <td>string</td>\n",
    "            <td>driver ID</td>\n",
    "            <td>56018323b921dd2c5444f98fb45509de</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>passenger_id</td>\n",
    "            <td>string</td>\n",
    "            <td>user ID</td>\n",
    "            <td>238de35f44bbe8a67bdea86a5b0f4719</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>start_district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>departure</td>\n",
    "            <td>d4ec2125aff74eded207d2d915ef682f</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>dest_district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>destination</td>\n",
    "            <td>929ec6c160e6f52c20a4217c7978f681</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Price</td>\n",
    "            <td>double</td>\n",
    "            <td>Price</td>\n",
    "            <td>37.5</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>string</td>\n",
    "            <td>Timestamp of the order</td>\n",
    "            <td>2016-01-15 00:35:11</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The Order Info Table shows the basic information of an order, including the passenger and the driver (if driver_id =NULL, it means the order was not answered by any driver), place of origin, destination, price and time. The fields order_id, driver_id, passenger_id, start_hash, and dest_hash are made not sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Columns in order files\n",
    "columns = ['order_id', 'driver_id', 'passenger_id', 'start_district_hash', 'dest_district_hash', 'price', 'time']\n",
    "\n",
    "# Open only one file\n",
    "# order_file_1 = \"data/season_1/training_data/order_data/order_data_2016-01-01\"\n",
    "# df = df_1 = pd.read_csv(order_file_1, sep = \"\\t\", names = columns, parse_dates = 'time')\n",
    "\n",
    "# Files are organized by dates\n",
    "# n_files = 22\n",
    "# order_files = [\"data/season_1/training_data/order_data/order_data_2016-01-{:02d}\".format(i) \n",
    "#                for i in range(1, n_files)]\n",
    "order_files = [\"data/season_1/training_data/order_data/order_data_2016-01-{:02d}\".format(i) \n",
    "               for i in range(10, 16)]\n",
    "\n",
    "\n",
    "# Open all of them\n",
    "order_dfs = []\n",
    "for order_file in order_files:\n",
    "    order_dfs.append(pd.read_csv(order_file, sep = \"\\t\", names = columns))\n",
    "df = pd.concat(order_dfs)\n",
    "\n",
    "# Recognize time column as time\n",
    "df['time'] = pd.to_datetime(df.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Attach time-related info \n",
    "\n",
    "def compute_timeinfo(df):\n",
    "    # Input: data frame with time column.\n",
    "    \n",
    "    # Compute 10-min timeslots -- absolute and per day.\n",
    "    df['timeslot_absolute'] = (df['time'] - pd.to_datetime('2016-01-01')).astype('timedelta64[m]')//10\n",
    "    df['timeslot_day'] = df['timeslot_absolute'] % (24 * 6)\n",
    "    \n",
    "    # Determine day of week, weekend.\n",
    "    df['dow'] = df.time.dt.dayofweek\n",
    "    df['weekend'] = df.dow >= 5\n",
    "    \n",
    "compute_timeinfo(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set for training, validating, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep first two weeks for training, next one week for validation.\n",
    "ind = df['time'] < pd.to_datetime('2016-01-15')\n",
    "df_train = df[ind]\n",
    "df_valid = df[~ind]\n",
    "\n",
    "# Avoid looking at validation set during the exploration\n",
    "df = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open list of slots for test set\n",
    "order_file = 'data/season_1/test_set_1/read_me_1.txt'\n",
    "df_test = pd.read_csv(order_file, sep = \"\\t\", names = ['datetimeslot'], skiprows = 1)\n",
    "\n",
    "# Extract date and timeslot [0,143]\n",
    "df_test['date'] = pd.to_datetime(df_test.datetimeslot.str[:10])\n",
    "df_test['timeslot'] = df_test.datetimeslot.str[11:]\n",
    "df_test['timeslot'] = df_test.timeslot.astype(int)\n",
    "\n",
    "# Day of week\n",
    "df_test['dow'] = df_test.date.dt.dayofweek\n",
    "df_test['weekend'] = df_test.dow >= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_gap(df):\n",
    "    # Compute gap per time slot per district\n",
    "    # Input: data frame with ['driver_id', 'timeslot_absolute', 'start_district_hash']\n",
    "    \n",
    "    cols = ['start_district_hash', 'timeslot_absolute']\n",
    "    \n",
    "    df['is_gap'] = df['driver_id'].isnull()\n",
    "    df_grouped = df.groupby(cols)\n",
    "    df['gap'] = df_grouped['is_gap'].transform('sum')\n",
    "    \n",
    "# Apply to training and validation set\n",
    "compute_gap(df_train)\n",
    "compute_gap(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rolling_thing_by_basis(thing, basis, window, func, *args, **kwargs):\n",
    "    # Roll function funct over thing column by basis column for a window range [a,b] with a<b.\n",
    "    # Indexing makes rolling faster, but basis must be sorted.\n",
    "    \n",
    "    # http://stackoverflow.com/questions/14300768/\n",
    "    # pandas-rolling-computation-with-window-based-on-values-instead-of-counts\n",
    "\n",
    "    indexed_thing = pd.Series(thing.values, index = basis.values)\n",
    "    \n",
    "    def apply_window(val):\n",
    "        # slice_indexer instead of thing.loc[val:val+window] allows window limits not in the index\n",
    "        indexer = indexed_thing.index.slice_indexer(val + window[0], val + window[1], 1)\n",
    "        chunk = indexed_thing[indexer]\n",
    "        return func(chunk, *args, **kwargs)\n",
    "    \n",
    "    rolled = basis.apply(apply_window)\n",
    "    return rolled\n",
    "\n",
    "d = pd.DataFrame({'gap': [10,11,12,13], 'timeslot': [1,2,3,4]}).sort_values(by = 'timeslot')\n",
    "\n",
    "# Sum the prior 3 entries\n",
    "d['applied'] = rolling_thing_by_basis(d.gap, d.timeslot, [-3, -1], np.sum)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rolling_thing_by_basis(thing, basis, window, func):\n",
    "    # Roll function funct over thing column by basis column for a window range [a,b] with a<b.\n",
    "    # http://stackoverflow.com/questions/14300768/\n",
    "    # pandas-rolling-computation-with-window-based-on-values-instead-of-counts\n",
    "    \n",
    "    def apply_window(val):        \n",
    "        # chunk = thing[(val+window[0] <= basis) & (basis <= val+window[1])]\n",
    "        chunk = thing[(basis >= val+window[0]) & (basis <= val+window[1])]\n",
    "        # chunk = thing[(basis <= val) & (basis >= val-3)]\n",
    "        return func(chunk)\n",
    "    \n",
    "    return basis.apply(apply_window)\n",
    "\n",
    "d = pd.DataFrame({'gap': [10,11,12,13], 'timeslot': [1,2,3,4]})\n",
    "\n",
    "# Sum the prior 3 entries\n",
    "d['applied'] = rolling_thing_by_basis(d.gap, d.timeslot, [-3, -1], np.sum)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute rolling average\n",
    "\n",
    "def compute_prior_mean(df):\n",
    "    df = df.sort_values(by = 'timeslot_absolute')    \n",
    "    df_grouped = df.groupby('start_district_hash')\n",
    "    \n",
    "    # df['gap_mean_prior'] = df_grouped.apply(rolling_thing_by_basis, \n",
    "    #                                         df_grouped['gap'], df_grouped['timeslot_absolute'], [-3, -1], np.mean)\n",
    "    \n",
    "    # df['gap_mean_prior'] = \n",
    "    df_grouped.apply(\n",
    "        lambda x: rolling_thing_by_basis(df_grouped.get_group(x).gap, df_grouped.get_group(x).timeslot_absolute, [-3, -1], np.mean))\n",
    "    \n",
    "    # df['gap_mean_prior'] = df_grouped.gap.apply(pd.rolling_sum, window = 4, min_periods = 1)\n",
    "    # df['gap_mean_prior'] = (df['gap_mean_prior'] - df['gap'])/3\n",
    "    \n",
    "    # df['gap_mean_prior'] = df_grouped.gap.apply(pd.rolling, window = 4, win_type = [1, 1, 1, 0]).mean()\n",
    "    \n",
    "    # df_pivoted = pd.pivot_table(df, index = 'timeslot_absolute', columns = 'start_district_hash', values = 'gap')\n",
    "    # return df_pivoted.rolling(window = 4, win_type = [1,1,1,0]).mean().stack\n",
    "\n",
    "    # def f(x):\n",
    "    #     filtered = df_grouped.gap[df_grouped.timeslot < x & df_grouped.timeslot >= x-3]\n",
    "    #     return filtered.sum()\n",
    "\n",
    "df_train.head(10)\n",
    "compute_prior_mean(df_train)\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['start_district_hash', 'timeslot_absolute']\n",
    "df_sorted = df.sort_values(by = 'timeslot_absolute')    \n",
    "df_grouped = df_sorted.groupby(cols)['gap'].mean().reset_index()\n",
    "df_grouped = df_grouped.groupby('start_district_hash')\n",
    "df_grouped['gap_mean_prior'] = df_grouped['gap'].apply(pd.rolling_mean, window = 3)\n",
    "df_grouped['gap_mean_prior'] = df_grouped['gap_mean_prior'] / 3\n",
    "# df_grouped['gap_mean_prior'][df_rolling.isnull()]\n",
    "df_rolling = df_grouped['gap_mean_prior'].fillna(method = 'bfill')\n",
    "df_rolling = df_rolling.reset_index()\n",
    "# df['gap_mean_prior'] = df_rolling\n",
    "df.drop('start_district_hash', axis = 1, inplace = True)\n",
    "df.merge(df_rolling[cols + ['gap_mean_prior']], by = cols, on = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time</td>\n",
    "            <td>string</td>\n",
    "            <td>Timestamp</td>\n",
    "            <td>2016-01-15 00:35:11</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Weather</td>\n",
    "            <td>int</td>\n",
    "            <td>Weather</td>\n",
    "            <td>7</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>temperature</td>\n",
    "            <td>double</td>\n",
    "            <td>Temperature</td>\n",
    "            <td>-9</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>PM2.5</td>\n",
    "            <td>double</td>\n",
    "            <td>pm25</td>\n",
    "            <td>66</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The Weather Info Table shows the weather info every 10 minutes each city. The weather field gives the weather conditions such as sunny, rainy, and snowy etc; all sensitive information has been removed. The unit of temperature is Celsius degree, and PM2.5 is the level of air pollutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Files are organized by dates\n",
    "n_files = 21\n",
    "weather_files = [\"data/season_1/training_data/weather_data/weather_data_2016-01-{:02d}\".format(i)\n",
    "                 for i in range(1, n_files)]\n",
    "\n",
    "# Open all of them\n",
    "columns = ['time', 'weather', 'temperature', 'pm25']\n",
    "weather_dfs = []\n",
    "for f in weather_files:\n",
    "    weather_dfs.append(pd.read_csv(f, sep = \"\\t\", names = columns))\n",
    "dfw = pd.concat(weather_dfs)\n",
    "\n",
    "# Extract date and timeslot\n",
    "dfw['time'] = pd.to_datetime(dfw.time)\n",
    "compute_timeinfo(dfw)\n",
    "\n",
    "# Merge into main data frame, and fill missing values\n",
    "# http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
    "df = df.merge(dfw, on = ['timeslot_absolute'], how = 'left')\n",
    "df.temperature = df.temperature.fillna(method = 'ffill') # forward fill\n",
    "# df.temperature = df.temperature.interpolate(method = 'time') # time-based interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of most popular districts to keep\n",
    "k = 10\n",
    "\n",
    "# Rank\n",
    "df['start_district_count'] = df.groupby('start_district_hash')['start_district_hash'].transform('count')\n",
    "df['start_district_rank'] = df['start_district_count'].rank(ascending = False, method = 'dense')\n",
    "\n",
    "# Extract most popular districts\n",
    "num_entries = df.shape[0]\n",
    "df_filtered = df[df['start_district_rank'] <= k]\n",
    "num_top = df_filtered.groupby('start_district_hash')['start_district_count'].mean().sum()\n",
    "districts_top = df_filtered['start_district_hash']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute time slot\n",
    "compute_timeinfo(dfw)\n",
    "\n",
    "# Compute gap per time slot per district\n",
    "compute_gap(df_valid)\n",
    "\n",
    "# Merge temperature\n",
    "df_valid = df_valid.merge(dfw, on = ['timeslot_absolute'], how = 'left')\n",
    "df_valid.temperature = df_valid.temperature.fillna(method = 'ffill')\n",
    "\n",
    "# One-hot encoding of districts\n",
    "# df_valid['district'] = np.nan\n",
    "# df_valid.loc[df_valid.start_district_hash.isin(districts), 'district'] = \\\n",
    "#     df_valid.loc[df_valid.start_district_hash.isin(districts), 'start_district_hash']\n",
    "# dummies = pd.get_dummies(df_valid['district'], dummy_na = False)\n",
    "# df_valid = pd.concat((df_valid.drop('district', axis = 1), dummies.astype(int)), axis = 1)\n",
    "\n",
    "# Replace district by popularity\n",
    "districts_rank = df[['start_district_hash', 'district_rank']]\n",
    "districts_rank = districts_rank.drop_duplicates(subset = ['start_district_hash'], keep = 'first')\n",
    "districts_rank = districts_rank.set_index('start_district_hash')['district_rank']\n",
    "df_valid['start_district_rank'] = districts_rank[df_valid.start_district_hash].reset_index()['district_rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions by Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make first prediction by simply taking the prior mean per start_district_hash\n",
    "train_outcome = df_cluster['gap']\n",
    "train_predict = df_cluster['gap_mean_prior']\n",
    "\n",
    "# Validation\n",
    "valid_outcome = df_valid_cluster['gap']\n",
    "valid_predict = df_valid_cluster['gap_mean_prior']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select features\n",
    "cols = ['start_district_rank', 'dow', 'timeslot', 'temperature', 'gap_mean_prior']\n",
    "# cols = ['start_district_rank', 'weekend', 'timeslot', 'temperature', 'gap_mean_prior']\n",
    "train = df[cols]\n",
    "\n",
    "# Select regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor(n_estimators = 10)\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# reg = DecisionTreeRegressor(max_depth = 3)\n",
    "\n",
    "# Fit training data\n",
    "start = clock()\n",
    "reg.fit(train, train_outcome)\n",
    "print(\"Fit in {:.0f} seconds.\".format(clock() - start))\n",
    "\n",
    "# Extrapolate to test data\n",
    "start = clock()\n",
    "train_predict = reg.predict(train)\n",
    "print(\"Extrapolate in {:.0f} seconds.\".format(clock() - start))\n",
    "\n",
    "# Reference outcome\n",
    "train_outcome = df['gap']\n",
    "\n",
    "# Validation\n",
    "valid_outcome = df_valid['gap']\n",
    "valid_predict = reg.predict(df_valid[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Consider di districts and tj time slots, and the supply-demand gap gapij , and your prediction is sij, we use as the evaluation metrics: \n",
    "![MAPE](figures/mape.jpg)\n",
    "The lowest MAPE will be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(outcome, predict):\n",
    "    # Compute MAPE score. Lower is better.\n",
    "    import numpy as np\n",
    "    \n",
    "    # Compute errors summand for summand with nonzero denominator\n",
    "    diff = (outcome - predict) / outcome.replace({0: np.nan})\n",
    "    diff = diff.replace({np.nan: 0})\n",
    "    diff = diff.abs()\n",
    "    \n",
    "    # Compute the average over all district and timeslots for which outcome is NONZERO\n",
    "    nq = len(outcome.nonzero()[0])\n",
    "    return diff.sum() / nq\n",
    "\n",
    "# As of June 7th...\n",
    "# lowest score online is 0.224257,\n",
    "# 100th is 0.27747,\n",
    "# 500th is 0.360159.\n",
    "\n",
    "score = mape(train_outcome, train_predict)\n",
    "print(\"Training MAPE: {:.6f}\".format(score))\n",
    "\n",
    "score = mape(valid_outcome, valid_predict)\n",
    "print(\"Validation MAPE: {:.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# District Info Table\n",
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <th>Field</th>\n",
    "            <th>Type</th>\n",
    "            <th>Meaning</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>district_hash</td>\n",
    "            <td>string</td>\n",
    "            <td>District hash</td>\n",
    "            <td>90c5a34f06ac86aee0fd70e2adce7d8a</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>district_id</td>\n",
    "            <td>string</td>\n",
    "            <td>District ID</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "</table>\n",
    "\n",
    "The District Info Table shows the information about the districts to be evaluated in the contest. You need to do the prediction given the districts from the District Definition Table. In the submission of the results, you need to map the district hash value to district mapped ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the starting district_hash as the associated disctrict\n",
    "district_col = 'start_district_hash'\n",
    "\n",
    "# Load district conversion table\n",
    "district_file = 'data/season_1/training_data/cluster_map/cluster_map'\n",
    "district = pd.read_csv(district_file, sep = '\\t', names = [district_col, 'district_id'])\n",
    "\n",
    "# How many districts?\n",
    "print(\"There are {} districts in the district file.\".format(district.shape[0]))\n",
    "\n",
    "# Replace district_hash by district_id in data frame\n",
    "df = df.merge(district, on = district_col, how = 'left')\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "<table class=\"table table-2\">\n",
    "        <tr>\n",
    "            <th>Data name</th>\n",
    "            <th>Data type</th>\n",
    "            <th>Example</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>District ID</td>\n",
    "            <td>string</td>\n",
    "            <td>1,2,3,4 (the same as district mapping ID)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Time slot</td>\n",
    "            <td>string</td>\n",
    "            <td>2016-01-23-1 (The first time slot on Jan. 23rd, 2016; one day is uniformly divided into 144 ten minute time slots)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Prediction value</td>\n",
    "            <td>double</td>\n",
    "            <td>6.0</td>\n",
    "        </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select prediction\n",
    "col_predict = 'gap_mean_prior'\n",
    "df_test = df\n",
    "\n",
    "# Compute time slot\n",
    "# One day is uniformly divided into 144 ten minute time slots. Indexed from 1 to 144.\n",
    "df_test['timeslot_output'] = df_test['timeslot_day'] + 1\n",
    "\n",
    "# Make the date - timeslot column\n",
    "df_test['datetimeslot'] = df_test.date.map(str) + '-' + df_test.timeslot_output.astype(int).map(str)\n",
    "\n",
    "# Prepare output file\n",
    "cols = ['district_id', 'datetimeslot']\n",
    "final = df_test[cols + [col_predict]].groupby(cols).mean().reset_index()\n",
    "final.to_csv(\"predict.csv\", index = False, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
